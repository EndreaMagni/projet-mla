{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferie\\Anaconda3\\envs\\ISI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=load_from_disk('tokenized_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset=tokenized_data['train'].select(range(40000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping 30000 commun words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_commun(example,eng_vocab,fr_vocab):\n",
    "\n",
    "    # Tokenize\n",
    "    example['translation']['en'] =  [word if word in eng_vocab else 'UNK' for word in example['translation']['en']]\n",
    "    example['translation']['fr'] = [word if word in fr_vocab else 'UNK' for word in example['translation']['fr']]\n",
    "\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "eng_vocab_path= '../../30k_eng.txt'  # Replace with your file path\n",
    "fr_vocab_path = '../../30k_fr.txt'\n",
    "# Open the file and read lines into a list\n",
    "with open(eng_vocab_path, 'r') as file:\n",
    "    eng_vocab = [line.strip() for line in file]\n",
    "\n",
    "with open(fr_vocab_path, 'r') as file:\n",
    "    fr_vocab = [line.strip() for line in file]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD\n",
      "['resumption', 'of', 'the', 'session']\n",
      "['UNK', 'of', 'the', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab[0] )\n",
    "test_commun=most_commun(mini_dataset[0],eng_vocab,fr_vocab)\n",
    "print(mini_dataset[0]['translation']['en'])\n",
    "print(test_commun['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset=mini_dataset.map(most_commun, fn_kwargs={\"eng_vocab\": eng_vocab, \"fr_vocab\": fr_vocab}, num_proc=12)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['UNK', 'of', 'the', 'session'], 'fr': ['reprise', 'de', 'la', 'session']}}\n"
     ]
    }
   ],
   "source": [
    "print(mini_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(example, max_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates a sentence to a specific length.\n",
    "\n",
    "    Args:\n",
    "    - sentence (list): The sentence to pad, represented as a list of tokens/words.\n",
    "    - max_length (int): The maximum length of the sentence.\n",
    "    - padding_token (int or str, optional): The token used for padding shorter sentences.\n",
    "\n",
    "    Returns:\n",
    "    - list: The padded or truncated sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Truncate the sentence if it's longer than max_length\n",
    "    if len(example['translation']['en']) > max_length:\n",
    "       example['translation']['en']=example['translation']['en'][:max_length]\n",
    "    # Pad the sentence if it's shorter than max_length\n",
    "    else :\n",
    "        example['translation']['en'] = example['translation']['en'] + ['PAD']*(max_length-len(example['translation']['en']))\n",
    "\n",
    "    if len(example['translation']['fr']) > max_length:\n",
    "        example['translation']['fr']=example['translation']['fr'][:max_length]\n",
    "    else :\n",
    "        example['translation']['fr'] = example['translation']['fr'] + ['PAD']*(max_length-len(example['translation']['fr']))  \n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['UNK', 'of', 'the', 'session'], 'fr': ['reprise', 'de', 'la', 'session']}}\n",
      "{'translation': {'en': ['UNK', 'of', 'the', 'session', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], 'fr': ['reprise', 'de', 'la', 'session', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_padding=pad_sentence(mini_dataset[0], 10)\n",
    "print(mini_dataset[0])\n",
    "print(test_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset=mini_dataset.map(pad_sentence, fn_kwargs={\"max_length\": 50}, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_dataset[10]['translation']['en']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cleaning words and creating the dictionary\n",
    "word_dict_fr = {i: word.strip() for i, word in enumerate(fr_vocab)}\n",
    "\n",
    "# Showing the first 10 entries of the dictionary as an example\n",
    "example_dict = {k: word_dict_fr[k] for k in list(word_dict_fr)[:10]}\n",
    "example_dict\n",
    "print(len(word_dict_fr))\n",
    "#eng dictionnary \n",
    "word_dict_eng = {i: word.strip() for i, word in enumerate(eng_vocab)}\n",
    "print(len(word_dict_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 32000\n",
      "Test set size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets\n",
    "train_test_split_ratio = 0.2  # 20% for testing\n",
    "train_dataset, test_dataset = padded_dataset.train_test_split(test_size=train_test_split_ratio).values()\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(padded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 40000/40000 [00:00<00:00, 523137.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "padded_dataset.save_to_disk(\"mini_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk('C:\\\\Users\\\\ferie\\\\OneDrive\\\\Bureau\\\\M2 ISI\\\\mini_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['UNK', 'of', 'the', 'session', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], 'fr': ['reprise', 'de', 'la', 'session', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']}}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pairs\n",
    "pairs = [(item['translation']['en'], item['translation']['fr']) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(pairs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the pairs into batches\n",
    "batch_size = 80\n",
    "batches = [pairs[i:i + batch_size] for i in range(0, len(pairs), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "eng_vocab_path= '../../30k_eng.txt'  # Replace with your file path\n",
    "fr_vocab_path = '../../30k_fr.txt'\n",
    "# Open the file and read lines into a list\n",
    "with open(eng_vocab_path, 'r') as file:\n",
    "    eng_vocab = [line.strip() for line in file]\n",
    "\n",
    "with open(fr_vocab_path, 'r') as file:\n",
    "    fr_vocab = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Cleaning words and creating the dictionary\n",
    "word_dict_fr = {i: word.strip() for i, word in enumerate(fr_vocab)}\n",
    "\n",
    "\n",
    "print(len(word_dict_fr))\n",
    "#eng dictionnary \n",
    "word_dict_eng = {i: word.strip() for i, word in enumerate(eng_vocab)}\n",
    "print(len(word_dict_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "print(word_dict_eng['as'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26684\n"
     ]
    }
   ],
   "source": [
    "# Creating word_to_id dictionary for French vocabulary\n",
    "word_to_id_fr = {word.strip(): i for i, word in enumerate(fr_vocab)}\n",
    "\n",
    "print(len(word_to_id_fr))\n",
    "\n",
    "# Creating word_to_id dictionary for English vocabulary\n",
    "word_to_id_eng = {word.strip(): i for i, word in enumerate(eng_vocab)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pairs\n",
    "pairs = [(item['translation']['en'], item['translation']['fr']) for item in data]\n",
    "#creating batches \n",
    "batches = [pairs[i:i + batch_size] for i in range(0, len(pairs), batch_size)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def make_batch(data,batch_size):\n",
    "    # creating pairs\n",
    "    pairs = [(item['translation']['en'], item['translation']['fr']) for item in data]\n",
    "    random.shuffle(pairs)\n",
    "    batches = [pairs[i:i + batch_size] for i in range(0, len(pairs), batch_size)]\n",
    "\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches =make_batch(data,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 80, 2, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.shape(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=30000\n",
    "input_batch=[]\n",
    "output_batch=[]\n",
    "\n",
    "for batch in batches : \n",
    "    for pair in batch: \n",
    "        input_batch.append([np.eye(vocab_size)[[word_to_id_eng[n] for n in pair[0]]]])\n",
    "        output_batch.append([np.eye(vocab_size)[[word_to_id_fr[n] for n in pair[1]]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
