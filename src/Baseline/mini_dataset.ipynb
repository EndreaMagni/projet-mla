{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferie\\Anaconda3\\envs\\ISI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=load_from_disk('tokenized_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset=tokenized_data['train'].select(range(40000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping 30000 commun words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_commun(example,eng_vocab,fr_vocab):\n",
    "\n",
    "    # Tokenize\n",
    "    example['translation']['en'] =  [word if word in eng_vocab else 'UNK' for word in example['translation']['en']]\n",
    "    example['translation']['fr'] = [word if word in fr_vocab else 'UNK' for word in example['translation']['fr']]\n",
    "\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "eng_vocab_path= '../../30k_eng.txt'  # Replace with your file path\n",
    "fr_vocab_path = '../../30k_fr.txt'\n",
    "# Open the file and read lines into a list\n",
    "with open(eng_vocab_path, 'r') as file:\n",
    "    eng_vocab = [line.strip() for line in file]\n",
    "\n",
    "with open(fr_vocab_path, 'r') as file:\n",
    "    fr_vocab = [line.strip() for line in file]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD\n",
      "['resumption', 'of', 'the', 'session']\n",
      "['UNK', 'of', 'the', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab[0] )\n",
    "test_commun=most_commun(mini_dataset[0],eng_vocab,fr_vocab)\n",
    "print(mini_dataset[0]['translation']['en'])\n",
    "print(test_commun['translation']['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset=mini_dataset.map(most_commun, fn_kwargs={\"eng_vocab\": eng_vocab, \"fr_vocab\": fr_vocab}, num_proc=12)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['UNK', 'of', 'the', 'session'], 'fr': ['reprise', 'de', 'la', 'session']}}\n"
     ]
    }
   ],
   "source": [
    "print(mini_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(example, max_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates a sentence to a specific length.\n",
    "\n",
    "    Args:\n",
    "    - sentence (list): The sentence to pad, represented as a list of tokens/words.\n",
    "    - max_length (int): The maximum length of the sentence.\n",
    "    - padding_token (int or str, optional): The token used for padding shorter sentences.\n",
    "\n",
    "    Returns:\n",
    "    - list: The padded or truncated sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Truncate the sentence if it's longer than max_length\n",
    "    if len(example['translation']['en']) > max_length:\n",
    "       example['translation']['en']=example['translation']['en'][:max_length]\n",
    "    # Pad the sentence if it's shorter than max_length\n",
    "    else :\n",
    "        example['translation']['en'] = example['translation']['en'] + ['PAD']*(max_length-len(example['translation']['en']))\n",
    "\n",
    "    if len(example['translation']['fr']) > max_length:\n",
    "        example['translation']['fr']=example['translation']['fr'][:max_length]\n",
    "    else :\n",
    "        example['translation']['fr'] = example['translation']['fr'] + ['PAD']*(max_length-len(example['translation']['fr']))  \n",
    "    \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['UNK', 'of', 'the', 'session'], 'fr': ['reprise', 'de', 'la', 'session']}}\n",
      "{'translation': {'en': ['UNK', 'of', 'the', 'session', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], 'fr': ['reprise', 'de', 'la', 'session', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_padding=pad_sentence(mini_dataset[0], 10)\n",
    "print(mini_dataset[0])\n",
    "print(test_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset=mini_dataset.map(pad_sentence, fn_kwargs={\"max_length\": 50}, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_dataset[10]['translation']['en']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cleaning words and creating the dictionary\n",
    "word_dict_fr = {i: word.strip() for i, word in enumerate(fr_vocab)}\n",
    "\n",
    "# Showing the first 10 entries of the dictionary as an example\n",
    "example_dict = {k: word_dict_fr[k] for k in list(word_dict_fr)[:10]}\n",
    "example_dict\n",
    "print(len(word_dict_fr))\n",
    "#eng dictionnary \n",
    "word_dict_eng = {i: word.strip() for i, word in enumerate(eng_vocab)}\n",
    "print(len(word_dict_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 32000\n",
      "Test set size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets\n",
    "train_test_split_ratio = 0.2  # 20% for testing\n",
    "train_dataset, test_dataset = padded_dataset.train_test_split(test_size=train_test_split_ratio).values()\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(padded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 40000/40000 [00:00<00:00, 523137.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "padded_dataset.save_to_disk(\"mini_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ferie\\anaconda3\\envs\\isi\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ferie\\anaconda3\\envs\\isi\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ferie\\anaconda3\\envs\\isi\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.2 MB 2.3 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/9.2 MB 2.5 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.2/9.2 MB 2.5 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/9.2 MB 2.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/9.2 MB 2.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/9.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.2 MB 3.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.2/9.2 MB 3.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.4/9.2 MB 3.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/9.2 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/9.2 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.1/9.2 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/9.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.5/9.2 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.7/9.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.8/9.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.2/9.2 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.3/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.4/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.5/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.7/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.0/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.2/9.2 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.3/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.4/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.6/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.8/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.9/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.1/9.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.4/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.7/9.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.0/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.1/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.3/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.5/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.9/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.4/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.0/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.3/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.4/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.2 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.8/9.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.2/9.2 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(example, eng_word_to_idx, fr_word_to_idx):\n",
    "\n",
    "    # Extracting English and French sentences from the example\n",
    "    eng_sentence = example['translation']['en']\n",
    "    fr_sentence = example['translation']['fr']\n",
    "\n",
    "    # Initialize one-hot encoded vectors for the entire sentences\n",
    "    one_hot_encoded_sentence_en = np.zeros((len(eng_sentence), 30000))\n",
    "    one_hot_encoded_sentence_fr = np.zeros((len(fr_sentence), 30000))\n",
    "\n",
    "\n",
    "    # Vectorized one-hot encoding for English sentence\n",
    "    eng_indices = [eng_word_to_idx[word] for word in eng_sentence if word in eng_word_to_idx]\n",
    "    one_hot_encoded_sentence_en[np.arange(len(eng_sentence)), eng_indices] = 1\n",
    "\n",
    "    # Vectorized one-hot encoding for French sentence\n",
    "    fr_indices = [fr_word_to_idx[word] for word in fr_sentence if word in fr_word_to_idx]\n",
    "    one_hot_encoded_sentence_fr[np.arange(len(fr_sentence)), fr_indices] = 1\n",
    "\n",
    "    # Update the example with the one-hot encoded sentences\n",
    "    example['one_hot_encoded_en'] = one_hot_encoded_sentence_en.tolist()\n",
    "    example['one_hot_encoded_fr'] = one_hot_encoded_sentence_fr.tolist()\n",
    "\n",
    "    return example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
