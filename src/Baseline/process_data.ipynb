{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferie\\Anaconda3\\envs\\ISI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recuperé la base de données\n",
    "#only done once\n",
    "dataset = load_from_disk('wmt14_fr_en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:  29%|██▊       | 2/7 [39:28:42<98:41:47, 71061.41s/it]\n",
      "Saving the dataset (30/30 shards): 100%|██████████| 40836715/40836715 [07:29<00:00, 90909.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 180087.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3003/3003 [00:00<00:00, 132028.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset locally\n",
    "#only done once\n",
    "#dataset.save_to_disk(\"wmt14_fr_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 40836715\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multithread tokenization using map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesTokenizer\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_data(example: Dict[str, Any], mt_en: MosesTokenizer, mt_fr: MosesTokenizer) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenizes English and French text in the given dataset example.\n",
    "\n",
    "    :param example: A dictionary containing 'translation' key with 'en' and 'fr' subkeys.\n",
    "    :param mt_en: English MosesTokenizer instance.\n",
    "    :param mt_fr: French MosesTokenizer instance.\n",
    "    :return: The modified example with tokenized text.\n",
    "    \"\"\"\n",
    "    # Check if both 'en' and 'fr' keys exist\n",
    "    if 'translation' in example and 'en' in example['translation'] and 'fr' in example['translation']:\n",
    "        example['translation']['en'] = mt_en.tokenize(example['translation']['en'].lower())\n",
    "        example['translation']['fr'] = mt_fr.tokenize(example['translation']['fr'].lower())\n",
    "\n",
    "\n",
    "    return example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['would', 'it', 'be', 'appropriate', 'for', 'you', ',', 'madam', 'president', ',', 'to', 'write', 'a', 'letter', 'to', 'the', 'sri', 'lankan', 'president', 'expressing', 'parliament', '&apos;s', 'regret', 'at', 'his', 'and', 'the', 'other', 'violent', 'deaths', 'in', 'sri', 'lanka', 'and', 'urging', 'her', 'to', 'do', 'everything', 'she', 'possibly', 'can', 'to', 'seek', 'a', 'peaceful', 'reconciliation', 'to', 'a', 'very', 'difficult', 'situation', '?'], 'fr': ['ne', 'pensez-vous', 'pas', ',', 'madame', 'la', 'présidente', ',', 'qu&apos;', 'il', 'conviendrait', 'd&apos;', 'écrire', 'une', 'lettre', 'au', 'président', 'du', 'sri', 'lanka', 'pour', 'lui', 'communiquer', 'que', 'le', 'parlement', 'déplore', 'les', 'morts', 'violentes', ',', 'dont', 'celle', 'de', 'm.', 'ponnambalam', ',', 'et', 'pour', 'l&apos;', 'inviter', 'instamment', 'à', 'faire', 'tout', 'ce', 'qui', 'est', 'en', 'son', 'pouvoir', 'pour', 'chercher', 'une', 'réconciliation', 'pacifique', 'et', 'mettre', 'un', 'terme', 'à', 'cette', 'situation', 'particulièrement', 'difficile', '.']}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizers once, outside the function\n",
    "mt_en = MosesTokenizer(lang='en')\n",
    "mt_fr = MosesTokenizer(lang='fr')\n",
    "test_tokenization = tokenize_data(dataset['train'][10], mt_en, mt_fr)\n",
    "print(test_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "cpu_count = mp.cpu_count() \n",
    "print(cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_data, fn_kwargs={\"mt_en\": mt_en, \"mt_fr\": mt_fr}, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['that', 'is', 'why', 'my', 'group', 'moves', 'that', 'this', 'item', 'be', 'taken', 'off', 'the', 'agenda', '.'], 'fr': ['c&apos;', 'est', 'pourquoi', 'mon', 'groupe', 'demande', 'que', 'ce', 'point', 'soit', 'retiré', 'de', 'l&apos;', 'ordre', 'du', 'jour', '.']}}\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][100])\n",
    "print(type(tokenized_dataset))\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (47/47 shards): 100%|██████████| 40836715/40836715 [13:34<00:00, 50130.90 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 59020.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3003/3003 [00:00<00:00, 69604.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping 30000 most commun words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset=load_from_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "\n",
    "def create_vocab(example: Dict[str, Any], eng_counter: Counter, fr_counter: Counter) -> (Counter, Counter):\n",
    "    \"\"\"\n",
    "    Creates both English and French vocabularies from the given dataset example.\n",
    "\n",
    "    :param example: A dictionary containing 'translation' key with 'en' and 'fr' subkeys.\n",
    "    :param eng_counter: Counter object to store the English vocabulary.\n",
    "    :param fr_counter: Counter object to store the French vocabulary.\n",
    "    :return: The modified Counter objects for English and French vocabularies.\n",
    "    \"\"\"\n",
    "    if 'translation' in example:\n",
    "        if 'en' in example['translation']:\n",
    "            eng_counter.update(example['translation']['en'])\n",
    "        if 'fr' in example['translation']:\n",
    "            fr_counter.update(example['translation']['fr'])\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "en_counter = Counter()  \n",
    "fr_counter = Counter()\n",
    "\n",
    "# Create vocabularies from train data\n",
    "tokenized_dataset['train'].map(lambda example: create_vocab(example, en_counter, fr_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': ['would', 'it', 'be', 'appropriate', 'for', 'you', ',', 'madam', 'president', ',', 'to', 'write', 'a', 'letter', 'to', 'the', 'sri', 'lankan', 'president', 'expressing', 'parliament', '&apos;s', 'regret', 'at', 'his', 'and', 'the', 'other', 'violent', 'deaths', 'in', 'sri', 'lanka', 'and', 'urging', 'her', 'to', 'do', 'everything', 'she', 'possibly', 'can', 'to', 'seek', 'a', 'peaceful', 'reconciliation', 'to', 'a', 'very', 'difficult', 'situation', '?'], 'fr': ['ne', 'pensez-vous', 'pas', ',', 'madame', 'la', 'présidente', ',', 'qu&apos;', 'il', 'conviendrait', 'd&apos;', 'écrire', 'une', 'lettre', 'au', 'président', 'du', 'sri', 'lanka', 'pour', 'lui', 'communiquer', 'que', 'le', 'parlement', 'déplore', 'les', 'morts', 'violentes', ',', 'dont', 'celle', 'de', 'm.', 'ponnambalam', ',', 'et', 'pour', 'l&apos;', 'inviter', 'instamment', 'à', 'faire', 'tout', 'ce', 'qui', 'est', 'en', 'son', 'pouvoir', 'pour', 'chercher', 'une', 'réconciliation', 'pacifique', 'et', 'mettre', 'un', 'terme', 'à', 'cette', 'situation', 'particulièrement', 'difficile', '.']}}\n",
      "{'translation': {'en': ['and', 'while', 'congress', 'can', '&apos;t', 'agree', 'on', 'whether', 'to', 'proceed', ',', 'several', 'states', 'are', 'not', 'waiting', '.'], 'fr': ['et', 'tandis', 'que', 'les', 'membres', 'du', 'congrès', 'n&apos;', 'arrivent', 'pas', 'à', 'se', 'mettre', 'd&apos;', 'accord', 'pour', 'savoir', 's&apos;', 'il', 'faut', 'continuer', ',', 'plusieurs', 'états', 'n&apos;', 'ont', 'pas', 'attendu', '.']}}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data['train'][10])\n",
    "print(tokenized_data['test'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_commun(example,eng_vocab,fr_vocab):\n",
    "\n",
    "    if 'en' in example['translation'] and 'fr' in example['translation']:\n",
    "        # Tokenize\n",
    "        example['translation']['en'] =  [word if word in eng_vocab else 'UNK' for word in example['translation']['en']]\n",
    "        example['translation']['fr'] = [word if word in fr_vocab else 'UNK' for word in example['translation']['fr']]\n",
    "\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "eng_vocab_path= '../../30k_eng.txt'  # Replace with your file path\n",
    "fr_vocab_path = '../../30k_fr.txt'\n",
    "# Open the file and read lines into a list\n",
    "with open(eng_vocab_path, 'r') as file:\n",
    "    eng_vocab = [line.strip() for line in file]\n",
    "\n",
    "with open(fr_vocab_path, 'r') as file:\n",
    "    fr_vocab = [line.strip() for line in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30001\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_vocab))\n",
    "print(len(fr_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': \"Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\", 'fr': \"Ne pensez-vous pas, Madame la Présidente, qu'il conviendrait d'écrire une lettre au président du Sri Lanka pour lui communiquer que le Parlement déplore les morts violentes, dont celle de M. Ponnambalam, et pour l'inviter instamment à faire tout ce qui est en son pouvoir pour chercher une réconciliation pacifique et mettre un terme à cette situation particulièrement difficile.\"}}\n",
      "{'translation': {'en': ['would', 'it', 'be', 'appropriate', 'for', 'you', ',', 'madam', 'president', ',', 'to', 'write', 'a', 'letter', 'to', 'the', 'sri', 'lankan', 'president', 'expressing', 'parliament', '&apos;s', 'regret', 'at', 'his', 'and', 'the', 'other', 'violent', 'deaths', 'in', 'sri', 'lanka', 'and', 'urging', 'her', 'to', 'do', 'everything', 'she', 'possibly', 'can', 'to', 'seek', 'a', 'peaceful', 'reconciliation', 'to', 'a', 'very', 'difficult', 'situation', '?'], 'fr': ['ne', 'pensez-vous', 'pas', ',', 'madame', 'la', 'présidente', ',', 'qu&apos;', 'il', 'conviendrait', 'd&apos;', 'écrire', 'une', 'lettre', 'au', 'président', 'du', 'sri', 'lanka', 'pour', 'lui', 'communiquer', 'que', 'le', 'parlement', 'déplore', 'les', 'morts', 'violentes', ',', 'dont', 'celle', 'de', 'm.', 'ponnambalam', ',', 'et', 'pour', 'l&apos;', 'inviter', 'instamment', 'à', 'faire', 'tout', 'ce', 'qui', 'est', 'en', 'son', 'pouvoir', 'pour', 'chercher', 'une', 'réconciliation', 'pacifique', 'et', 'mettre', 'un', 'terme', 'à', 'cette', 'situation', 'particulièrement', 'difficile', '.']}}\n",
      "{'translation': {'en': ['would', 'it', 'be', 'appropriate', 'for', 'you', 'UNK', 'madam', 'president', 'UNK', 'to', 'write', 'a', 'letter', 'to', 'UNK', 'sri', 'lankan', 'president', 'expressing', 'parliament', 'UNK', 'regret', 'at', 'his', 'and', 'UNK', 'other', 'violent', 'deaths', 'in', 'sri', 'lanka', 'and', 'urging', 'her', 'to', 'do', 'everything', 'she', 'possibly', 'can', 'to', 'seek', 'a', 'peaceful', 'reconciliation', 'to', 'a', 'very', 'difficult', 'situation', 'UNK'], 'fr': ['ne', 'UNK', 'pas', 'UNK', 'madame', 'la', 'présidente', 'UNK', 'UNK', 'il', 'conviendrait', 'UNK', 'écrire', 'une', 'lettre', 'au', 'président', 'du', 'UNK', 'UNK', 'pour', 'lui', 'communiquer', 'que', 'le', 'parlement', 'déplore', 'les', 'morts', 'violentes', 'UNK', 'dont', 'celle', 'de', 'UNK', 'UNK', 'UNK', 'et', 'pour', 'UNK', 'inviter', 'UNK', 'à', 'faire', 'tout', 'ce', 'qui', 'est', 'en', 'son', 'pouvoir', 'pour', 'chercher', 'une', 'réconciliation', 'pacifique', 'et', 'mettre', 'un', 'terme', 'à', 'cette', 'situation', 'particulièrement', 'difficile', 'UNK']}}\n"
     ]
    }
   ],
   "source": [
    "test_vocab = most_commun(tokenized_dataset['train'][10],eng_vocab,fr_vocab)\n",
    "print(dataset['train'][10])\n",
    "print(tokenized_dataset['train'][10])\n",
    "print(test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = tokenized_dataset.map(most_commun, fn_kwargs={\"eng_vocab\": eng_vocab,\"fr_vocab\": fr_vocab }, num_proc=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(index, vocab_size):\n",
    "\n",
    "    # Initialize the list of one-hot encoded vectors\n",
    "    one_hot_vectors = np.zeros(vocab_size)\n",
    "    one_hot_vectors[index] = 1\n",
    "\n",
    "    return one_hot_vectors\n",
    "\n",
    "def one_hot_encode(example,eng_vocab,fr_vocab):\n",
    "    eng_vocab_size = len(eng_vocab)\n",
    "    fr_vocab_size = len(fr_vocab)\n",
    "\n",
    "    if 'en' in example['translation'] and 'fr' in example['translation']:\n",
    "        # Tokenize\n",
    "        example['translation']['en'] =  [one_hot_encode(eng_vocab.index(word),eng_vocab_size) for word in example['translation']['en']]\n",
    "        example['translation']['fr'] = [one_hot_encode(fr_vocab.index(word),fr_vocab_size) for word in example['translation']['fr']]\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_hot = one_hot_encode(updated_dataset['train'][10],eng_vocab,fr_vocab)\n",
    "print(test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_and_convert_to_integers(X, Y, max_length):\n",
    "    # Convert tokenized sentences to sequences of integers\n",
    "    tokenizer_X = Tokenizer()\n",
    "    tokenizer_X.fit_on_texts(X)\n",
    "    seq_X = tokenizer_X.texts_to_sequences(X)\n",
    "\n",
    "    tokenizer_Y = Tokenizer()\n",
    "    tokenizer_Y.fit_on_texts(Y)\n",
    "    seq_Y = tokenizer_Y.texts_to_sequences(Y)\n",
    "\n",
    "    # Pad sequences to a maximum length\n",
    "    padded_X = pad_sequences(seq_X, maxlen=max_length, padding='post')\n",
    "    padded_Y = pad_sequences(seq_Y, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded_X, padded_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing padding\n",
    "tokenized_X, tokenized_Y = tokenize_data(sample_data )\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_seq_length = 50\n",
    "\n",
    "# Pad the tokenized data and convert to integer sequences\n",
    "padded_X, padded_Y = pad_sequences_and_convert_to_integers(tokenized_X, tokenized_Y, max_seq_length)\n",
    "\n",
    "print(padded_X)\n",
    "print(padded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
